{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Nasıl Kullanılır?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- by: Bilalcan Ustabaş\n",
    "- linkedin: https://www.linkedin.com/in/bilalcanustabas/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gerekli Kütüphaneleri Yükleyelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri hazırlama kütüphaneleri ve fonksiyonları\n",
    "import pandas as pd # DataFrame işlemleri için gerekli kütüphane\n",
    "import numpy as np # Veri dönüşüm işlemleri için gerekli kütüphane\n",
    "from sklearn.datasets import make_classification # Örnek veri oluşturmamızı sağlayan fonksiyon\n",
    "\n",
    "# Veri ön işleme ve model otomatizasyonu sınıfları ve fonksiyonları\n",
    "from sklearn.pipeline import Pipeline # Yazımızın konusu olan otomatizasyon sınıfı\n",
    "from sklearn.model_selection import cross_val_score, train_test_split # Veri ayrıştırma ve cross validation yöntemleri için gerekli fonksiyonlar\n",
    "from sklearn.metrics import f1_score, roc_auc_score # Model performanslarını karşılaştırmak için kullanılan metrik fonksiyonları\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler # Preprocess işlemlerinde kullanılacak dönüşüm sınıfları\n",
    "\n",
    "# Deneylerde kullanılacak model sınıfları\n",
    "from sklearn.linear_model import LogisticRegression # Testler için kullanacağımız modellerden biri\n",
    "from sklearn.ensemble import RandomForestClassifier # Testler için kullanacağımız modellerden biri\n",
    "from lightgbm import LGBMClassifier # Testler için kullanacağımız modellerden biri\n",
    "from catboost import CatBoostClassifier # Testler için kullanacağımız modellerden biri\n",
    "\n",
    "# Deneylerde kullanılacak hiper parametre optimizasyon sınıfları ve kütüphaneleri\n",
    "from sklearn.model_selection import GridSearchCV # Hiper parametre deneyleri için gerekli sınıf\n",
    "import optuna # Hiper parametre deneyleri için gerekli kütüphane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Deney Hazırlığı"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Örnek Veri Oluşturulması"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 50000 satırlı %10 tahmin sınıf yüzdesine sahip 3 bilgi içeren 4 gereksiz 2 tekrarlı değişken içeren ve deneylerimizde kullanacağımız bir veri seti oluşturuyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================    X DataFrame   =========================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>col_10</th>\n",
       "      <th>col_11</th>\n",
       "      <th>col_12</th>\n",
       "      <th>col_13</th>\n",
       "      <th>col_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.317765</td>\n",
       "      <td>12.194548</td>\n",
       "      <td>5.107623</td>\n",
       "      <td>2.196658</td>\n",
       "      <td>4.691893</td>\n",
       "      <td>4.859987</td>\n",
       "      <td>2.511852</td>\n",
       "      <td>-0.861836</td>\n",
       "      <td>0.008581</td>\n",
       "      <td>1.364854</td>\n",
       "      <td>13.373359</td>\n",
       "      <td>3.219474</td>\n",
       "      <td>3.094977</td>\n",
       "      <td>5.901621</td>\n",
       "      <td>0.839221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.615254</td>\n",
       "      <td>13.178920</td>\n",
       "      <td>3.328998</td>\n",
       "      <td>1.748796</td>\n",
       "      <td>3.392489</td>\n",
       "      <td>6.321384</td>\n",
       "      <td>2.163355</td>\n",
       "      <td>1.332167</td>\n",
       "      <td>-0.993371</td>\n",
       "      <td>2.826251</td>\n",
       "      <td>13.357216</td>\n",
       "      <td>1.223225</td>\n",
       "      <td>2.438360</td>\n",
       "      <td>5.453758</td>\n",
       "      <td>3.675864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.103577</td>\n",
       "      <td>14.962391</td>\n",
       "      <td>4.841555</td>\n",
       "      <td>0.959526</td>\n",
       "      <td>1.047548</td>\n",
       "      <td>8.960307</td>\n",
       "      <td>1.462348</td>\n",
       "      <td>-0.529126</td>\n",
       "      <td>0.730165</td>\n",
       "      <td>5.465174</td>\n",
       "      <td>13.278040</td>\n",
       "      <td>-2.487800</td>\n",
       "      <td>1.606609</td>\n",
       "      <td>4.664488</td>\n",
       "      <td>1.561553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.379041</td>\n",
       "      <td>13.109531</td>\n",
       "      <td>2.999548</td>\n",
       "      <td>2.101685</td>\n",
       "      <td>4.638820</td>\n",
       "      <td>5.712265</td>\n",
       "      <td>0.973478</td>\n",
       "      <td>0.034586</td>\n",
       "      <td>-0.878587</td>\n",
       "      <td>2.217133</td>\n",
       "      <td>12.565993</td>\n",
       "      <td>0.299155</td>\n",
       "      <td>1.996482</td>\n",
       "      <td>5.806647</td>\n",
       "      <td>0.558182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.461640</td>\n",
       "      <td>12.113986</td>\n",
       "      <td>2.851236</td>\n",
       "      <td>2.692785</td>\n",
       "      <td>3.907561</td>\n",
       "      <td>4.959464</td>\n",
       "      <td>1.315312</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.115034</td>\n",
       "      <td>1.464331</td>\n",
       "      <td>12.412245</td>\n",
       "      <td>1.108321</td>\n",
       "      <td>1.555916</td>\n",
       "      <td>6.397747</td>\n",
       "      <td>2.320654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      col_0      col_1     col_2     col_3     col_4     col_5     col_6  \\\n",
       "0  4.317765  12.194548  5.107623  2.196658  4.691893  4.859987  2.511852   \n",
       "1  4.615254  13.178920  3.328998  1.748796  3.392489  6.321384  2.163355   \n",
       "2  5.103577  14.962391  4.841555  0.959526  1.047548  8.960307  1.462348   \n",
       "3  5.379041  13.109531  2.999548  2.101685  4.638820  5.712265  0.973478   \n",
       "4  4.461640  12.113986  2.851236  2.692785  3.907561  4.959464  1.315312   \n",
       "\n",
       "      col_7     col_8     col_9     col_10    col_11    col_12    col_13  \\\n",
       "0 -0.861836  0.008581  1.364854  13.373359  3.219474  3.094977  5.901621   \n",
       "1  1.332167 -0.993371  2.826251  13.357216  1.223225  2.438360  5.453758   \n",
       "2 -0.529126  0.730165  5.465174  13.278040 -2.487800  1.606609  4.664488   \n",
       "3  0.034586 -0.878587  2.217133  12.565993  0.299155  1.996482  5.806647   \n",
       "4  0.004511  0.115034  1.464331  12.412245  1.108321  1.555916  6.397747   \n",
       "\n",
       "     col_14  \n",
       "0  0.839221  \n",
       "1  3.675864  \n",
       "2  1.561553  \n",
       "3  0.558182  \n",
       "4  2.320654  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Target Imbalance =========================\n",
      "0 count: 44794\n",
      "1 count: 5206\n"
     ]
    }
   ],
   "source": [
    "# Oluşturulan verinin tekrar oluşturulabilmesi için seed belirlenmesi\n",
    "np.random.seed(42)\n",
    "\n",
    "# `make_classification` ile sınıflandırmada kullanılabilecek dengesiz hedef değişkenli 50000 satırlı verinin oluşturulması\n",
    "X, y = make_classification(n_samples=50000, n_features=15, \n",
    "                           n_informative=3, n_redundant=4, n_repeated=2, \n",
    "                           weights=(0.9,0.1), shift=[np.random.lognormal(mean=1) for _ in range(15)],\n",
    "                           random_state=42)\n",
    "\n",
    "# Dataframe olarak incelemek için dönüşüm ve kolon isimlendirmesi\n",
    "X = pd.DataFrame(X, columns=[f\"col_{i}\" for i in range(X.shape[1])])\n",
    "\n",
    "# Oluşturduğumuz verinin detaylarının basılması\n",
    "print(\"=\"*25, \"   X DataFrame  \", \"=\"*25)\n",
    "display(X.head())\n",
    "print(\"=\"*25, \"Target Imbalance\", \"=\"*25)\n",
    "print(f\"0 count: {(y==0).sum()}\")\n",
    "print(f\"1 count: {(y==1).sum()}\")\n",
    "\n",
    "# İşlem rahatlığı için dataframe'in numpy array'ine dönüştürülmesi\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Train-Test-Split İşlemlerinin Gerçekleştirilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eğitim ve test kümelerinin performanslarını ayrı ayrı incelemek için bölme işlemini gerçekleştiriyoruz\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Basic Pipeline Kurulumu ve Çalıştırılması"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pipeline sınıfı içerisinde veri dönüştüren sınıflar ve model tahmini gerçekleştirecek estimator'lar barındırabilir.\n",
    "- Pipeline içerisine yazılan Model içerisine verilebilen model parametreleri iletebiliriz. \n",
    "- Veri dönüşümü gerçekleştiren sınıfların tahmin gerçekleştiren estimator sınıflardından önce gelmesi gerekmektedir.\n",
    "- Tüm sınıfların kendi sınıf tiplerine göre içerilerinde belli fonksiyonları bulundurmak zorundadır. \n",
    "- Yapıya uyum sağlamayan sınıflar Pipeline'nın hata almasına sebep verirler.\n",
    "- İlerleyen bölümlerde sınıfların hangi fonksiyonları içermesi gerektiğini göreceğiz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Pipeline Kurulumu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocesser&#x27;, StandardScaler()),\n",
       "                (&#x27;estimator&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocesser&#x27;, StandardScaler()),\n",
       "                (&#x27;estimator&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocesser', StandardScaler()),\n",
       "                ('estimator', LogisticRegression())])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verilerimizi eğitim aşamasında kullanmadan önce yapmak istediğimiz basit bir ön işlem örneği olarak standardizasyon kullanalım\n",
    "# Model olarak LogisticRegression ile deneyimizi gerçekleştirelim\n",
    "pipe = Pipeline([\n",
    "        ('preprocesser', StandardScaler()), \n",
    "        ('estimator', LogisticRegression())\n",
    "        ])\n",
    "\n",
    "# Model .fit() edermiş gibi Pipeline sınıfı da içerisindeki aşamalarda bulunan .fit() fonksiyonlarını sırasıyla kullanır\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Pipeline ile Test Seti Üzerinde Veri Dönüşümü Gerçekleştirmek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- .fit() ile fit edilmiş bir Pipeline objesinin .transform() fonksiyonunu kullanarak eğitim seti haricindeki veri setleri de dönüştürülebilir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = pipe.named_steps['preprocesser'].transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Pipeline ile Tahmin Oluşturmak ve Değerlendirmek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tüm makine öğrenmesi modellerinde standartlaşmış .predict() ve .predict_proba() fonskiyonlarını Pipeline sınıfı ile kullanabiliyoruz. Bu sayede veri tahminlerini üretip değerlendirebiliriz.\n",
    "- .predict() ve .predict_proba() içerisine verilen verileri Pipwlinw boyu uygulanmış preprocess adımlarındaki tranformation'ları uygular, ekstra işleme gerek duyulmaz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tahminlemede kullanılacak bir fonksiyon oluşturalım\n",
    "def pipe_score(p, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # 0-1 tahminlerinin oluşturulması\n",
    "    y_pred_train = p.predict(X_train)\n",
    "    y_pred_test = p.predict(X_test)\n",
    "\n",
    "    # Tahmin proba'ların oluşturulması\n",
    "    y_pred_proba_train = p.predict_proba(X_train)[:,1]\n",
    "    y_pred_proba_test = p.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # Metrik hesaplanması\n",
    "    print(\"=\"*25, \"     Metrics    \", \"=\"*25)\n",
    "    print(f\"F1 Score: train: {f1_score(y_train, y_pred_train)}\")\n",
    "    print(f\"F1 Score: test: {f1_score(y_test, y_pred_test)}\")\n",
    "    print(f\"ROC-AUC Score: train: {roc_auc_score(y_train, y_pred_proba_train)}\")\n",
    "    print(f\"ROC-AUC Score: test: {roc_auc_score(y_test, y_pred_proba_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================      Metrics     =========================\n",
      "F1 Score: train: 0.804100858963702\n",
      "F1 Score: test: 0.8028872848417546\n",
      "ROC-AUC Score: train: 0.9110844241047773\n",
      "ROC-AUC Score: test: 0.9073245318527652\n"
     ]
    }
   ],
   "source": [
    "# Pipeline sınfı içerisnde kullanılan model (estimator) sınıflarının .predict() ve .predict_proba() fonskiyonlarını kullanabilir\n",
    "pipe_score(pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Pipeline ile Farklı Model Deneme Sürecinin Basitliği"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pipeline'nın içerisinde bulunan 'estimator' diye adlandırdığımız kısma başka bir model objesini koyarak aynı süreci başka bir model için tekrarlayabiliriz.\n",
    "- Pipeline içerisine yazılan Model içerisine verilebilen model parametreleri iletebiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================      Metrics     =========================\n",
      "F1 Score: train: 0.999879995199808\n",
      "F1 Score: test: 0.8620689655172414\n",
      "ROC-AUC Score: train: 0.999999993302799\n",
      "ROC-AUC Score: test: 0.9611378748612183\n"
     ]
    }
   ],
   "source": [
    "# Model olarak RandomForestClassifier ile deneyimizi gerçekleştirelim\n",
    "pipe_rf = Pipeline([\n",
    "        ('preprocesser', StandardScaler()), \n",
    "        ('estimator', RandomForestClassifier(**{'random_state':42}))\n",
    "        ])\n",
    "\n",
    "pipe_rf.fit(X_train, y_train)\n",
    "\n",
    "pipe_score(pipe_rf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================      Metrics     =========================\n",
      "F1 Score: train: 0.899756128866641\n",
      "F1 Score: test: 0.8583333333333333\n",
      "ROC-AUC Score: train: 0.9966410858370133\n",
      "ROC-AUC Score: test: 0.9649890193619467\n"
     ]
    }
   ],
   "source": [
    "# Model olarak LGBMClassifier ile deneyimizi gerçekleştirelim\n",
    "pipe_lgbm = Pipeline([\n",
    "        ('preprocesser', StandardScaler()), \n",
    "        ('estimator', LGBMClassifier(**{'random_state':42,\n",
    "                                        'verbose':-100}))\n",
    "        ])\n",
    "\n",
    "pipe_lgbm.fit(X_train, y_train)\n",
    "\n",
    "pipe_score(pipe_lgbm, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================      Metrics     =========================\n",
      "F1 Score: train: 0.9149126164051538\n",
      "F1 Score: test: 0.8535433070866143\n",
      "ROC-AUC Score: train: 0.9912930895983488\n",
      "ROC-AUC Score: test: 0.9638766168743843\n"
     ]
    }
   ],
   "source": [
    "# Model olarak CatBoostClassifier ile deneyimizi gerçekleştirelim\n",
    "pipe_cat = Pipeline([\n",
    "        ('preprocesser', StandardScaler()), \n",
    "        ('estimator', CatBoostClassifier(**{'random_state':42,\n",
    "                                            'verbose':0}))\n",
    "        ])\n",
    "\n",
    "pipe_cat.fit(X_train, y_train)\n",
    "\n",
    "pipe_score(pipe_cat, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Özelleştirilmiş Transformation Sınıflarının Yazılması"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Özelleştirilmiş Sınıfların Elemanları"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Özelleştirilmiş sınıfların, __init__, fit, transform fonksiyonlarını kesinlikle içermesi gerekir. Çünkü bu fonksiyonlar Pipeline'nın otomatizasyon sürücünde bulunmaktadır.\n",
    "- inverse_transform, fit_transform gibi fonksiyonlar gerekli olmamakla birlikte eklenebilir.\n",
    "- Bunlar dışındaki fonksiyonlar eklenebilir fakat otomatizasyon sürecine otomatik olarak dahil olmazlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bu özel sınıf kolonlara ayrı ayrı işlemler uygulayacak\n",
    "class CustomPreprocesser():\n",
    "    \n",
    "    # __init__ fonksiyonunda fit aşamasında verilecek parametreleri self ile çağırabilmek için None olarak tanımlıyoruz.\n",
    "    def __init__(self, X = None, ornek_parametre = None):\n",
    "        \n",
    "        self.X = X\n",
    "        self.ornek_parametre = ornek_parametre\n",
    "        \n",
    "    # .fit() fonksiyonu, Pipeline.fit() çağırıldığında gerçekleşir, fit() fonksiyonu içine vereceğimiz ekstra parametreler burada **params'ın içerisinde gelecektir.\n",
    "    ## Eklemek istediğiniz girdileri **params dictionary'sini döngü içinde aratarak elde edebilirsiniz.\n",
    "    ### .fit() fonksiyonunda transformation öncesi hazırlanma işlemlerinin yapılması gerekmektedir.\n",
    "    def fit(self, X, y = None, **params):\n",
    "\n",
    "        param_keys = list(params.keys()) # params içerisine gelen diğer parametreleri okuyoruz\n",
    "        for key in param_keys:\n",
    "            if \"ornek_parametre\" in key: # eğer istediğimiz isim bulunuyorsa istenilen yere kaydediyoruz\n",
    "                self.ornek_parametre = params[key]\n",
    "                 # transformation işlemleri için fitting işlemi\n",
    "\n",
    "        # return self kalmalı yoksa Pipeline yapısı hata veriyor\n",
    "        return self\n",
    "    \n",
    "    # .predict(), .predict_proba() öncesinde gerçekleşen transformation işlemlerini barındırır\n",
    "    ## .fit() içerisinde hazırlanmış veya hazırlanmamış tüm ön işlemler bu fonksiyon altında gerçekleşir\n",
    "    def transform(self, X):\n",
    "        \n",
    "        # eğer 'ornek_parametre' parametremiz mevcutsa transform işleminin yapılması\n",
    "        if self.ornek_parametre != None:\n",
    "            X[:,2] = self.ornek_parametre # örnek transform işlemi\n",
    "            \n",
    "        # herhangi bir parametrenin mevcutluğuna bağlı transform işlemi\n",
    "        ## burada istenilen tüm veri dönüşüm işlemleri gerçekleştirilebilir\n",
    "        X = \"Bazı veri işlemleri\"\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Gerçek Transform İşlemlerine Sahip Örnek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bu özel sınıf kolonlara ayrı ayrı işlemler uygulayacak\n",
    "class CustomPreprocesser():\n",
    "    \n",
    "    # __init__ fonksiyonunda fit aşamasında verilecek parametreleri self ile çağırabilmek için None olarak tanımlıyoruz.\n",
    "    def __init__(self, X = None, scaler_standard = None, scaler_min_max = None):\n",
    "        \n",
    "        self.X = X\n",
    "        self.scaler_standard = scaler_standard\n",
    "        self.scaler_min_max = scaler_min_max\n",
    "    \n",
    "    # .fit() fonksiyonu, Pipeline.fit() çağırıldığında gerçekleşir, fit() fonksiyonu içine vereceğimiz ekstra parametreler burada **params'ın içerisinde gelecektir.\n",
    "    ## Eklemek istediğiniz girdileri **params dictionary'sini döngü içinde aratarak elde edebilirsiniz.\n",
    "    ### .fit() fonksiyonunda transformation öncesi hazırlanma işlemlerinin yapılması gerekmektedir.\n",
    "    def fit(self, X, y = None, **params):\n",
    "\n",
    "        param_keys = list(params.keys()) # params içerisine gelen diğer parametreleri okuyoruz\n",
    "        for key in param_keys:\n",
    "            if \"scaler_standard\" in key: # eğer istediğimiz isim bulunuyorsa istenilen yere kaydediyoruz\n",
    "                self.scaler_standard = params[key]\n",
    "                self.scaler_standard.fit(X[:,2].reshape(-1, 1)) # transformation işlemleri için fitting işlemi\n",
    "            elif \"scaler_min_max\" in key: # eğer istediğimiz isim bulunuyorsa istenilen yere kaydediyoruz\n",
    "                self.scaler_min_max = params[key]\n",
    "                self.scaler_min_max.fit(X[:,1].reshape(-1, 1)) # transformation işlemleri için fitting işlemi\n",
    "        \n",
    "        # return self kalmalı yoksa Pipeline yapısı hata veriyor\n",
    "        return self\n",
    "    \n",
    "    # .predict(), .predict_proba() öncesinde gerçekleşen transformation işlemlerini barındırır\n",
    "    ## .fit() içerisinde hazırlanmış veya hazırlanmamış tüm ön işlemler bu fonksiyon altında gerçekleşir\n",
    "    def transform(self, X):\n",
    "        \n",
    "        # eğer 'scaler_standard' parametremiz mevcutsa transform işleminin yapılması\n",
    "        if self.scaler_standard != None:\n",
    "            X[:,2] = self.scaler_standard.transform(X[:,2].reshape(-1, 1)).ravel()\n",
    "            \n",
    "        # eğer 'scaler_min_max' parametremiz mevcutsa transform işleminin yapılması\n",
    "        if self.scaler_min_max != None:\n",
    "            X[:,1] = self.scaler_min_max.transform(X[:,1].reshape(-1, 1)).ravel()\n",
    "\n",
    "        # herhangi bir parametrenin mevcutluğuna bağlı transform işlemi\n",
    "        X[:,0] = np.sqrt(X[:,0])\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Özel Oluşturulmuş ve Parametre İhtiyacı Olan Sınıfların Kullanımı"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Oluşturduğumuz CustomPreprocesser sınıfında bulunan Scaler özelliklerini ayrı ayrı kullanmamız için onları Pipeline'nın .fit() kısmında parametre olarak vermemiz gerekiyor.\n",
    "- .fit() içine verilen parametreler şu şekilde yazılır: {Pipeline aşamasının ismi}__{parametre_ismi}={parametre_variable}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================      Metrics     =========================\n",
      "F1 Score: train: 0.901684292988641\n",
      "F1 Score: test: 0.8589341692789968\n",
      "ROC-AUC Score: train: 0.9934806231324897\n",
      "ROC-AUC Score: test: 0.9560995196917366\n"
     ]
    }
   ],
   "source": [
    "# Pipeline'nın kurulması\n",
    "pipe = Pipeline([\n",
    "        ('preprocesser', CustomPreprocesser()),\n",
    "        ('estimator', RandomForestClassifier())\n",
    "        ])\n",
    "\n",
    "pipe.fit(X_train, y_train,\n",
    "         preprocesser__scaler_standard=StandardScaler(), # Bu Pipeline örneği için preprocesser yukarıda CustomPreprocesser()'ın bulunduğu adımı işaret eder\n",
    "         preprocesser__scaler_min_max=MinMaxScaler()) # CustomPreprocesser() içinde belirlediğimiz parametre ismini de Pipeline adım ismi sonrasında '__' ile birleştirerek yazıyoruz\n",
    "\n",
    "pipe_score(pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pipeline ile Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross validation yöntemlerini kullanarak Pipeline'da belirlediğimiz tüm ön işlem ve model adımlarını train kümelerinde .fit() edip train ve validation kümelerinde .transform() yapmasını sağlayabiliyoruz.\n",
    "- Pipeline, içerisine model veya estimator olan tüm cross validation yöntemlerinde kullanılabilir.\n",
    "- Bu örnekte scikit-learn kütüphanesinin `cross_val_score` fonksiyonuna Pipeline objemizi vererek cross validation gerçekleştireceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation kümelerindeki validation roc-auc değerleri: [0.9232233  0.89186312 0.90091718 0.92350809 0.91176835]\n",
      "validation kümelerinde ortalama ve sapma roc-auc: mean:0.9102560083980812 std:0.012422296892597703\n"
     ]
    }
   ],
   "source": [
    "# Pipeline'ımızı oluşturuyoruz\n",
    "pipe = Pipeline([\n",
    "        ('preprocesser', CustomPreprocesser()),\n",
    "        ('estimator', LogisticRegression())\n",
    "        ])\n",
    "\n",
    "# Pipeline.fit() içerisine verdiğimiz parametreleri bir dictionary'e kaydedip daha sonra cross_val_score içerisinde kullanacağız\n",
    "fit_params = {\"preprocesser__scaler_standard\":StandardScaler(),\n",
    "              \"preprocesser__scaler_min_max\":MinMaxScaler()}\n",
    "\n",
    "# Estimator kısmına Pipeline objemizi verip diğer parametreleri de belirleyerek cross validation sürecimizi gerçekleştiriyoruz\n",
    "## fit_params parametresine yukarıda hazırladığımız ve Pipeline.fit() içerisinde kullanılacak parametreleri dictionary olarak veriyoruz\n",
    "cross_val_scores = cross_val_score(estimator=pipe, X=X, y=y, cv=5, scoring=\"roc_auc\", fit_params=fit_params)\n",
    "\n",
    "# Cross-Validation sonuçları\n",
    "print(\"validation kümelerindeki validation roc-auc değerleri:\", cross_val_scores)\n",
    "print(f\"validation kümelerinde ortalama ve sapma roc-auc: mean:{np.mean(cross_val_scores)} std:{np.std(cross_val_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Pipeline ile Hiper Parametre Optimizasyonu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Farklı Hiper Parametre optimizasyon yöntemleri farklı implementasyonlar gerektiriyor olsa da Pipeline objesinin kullanımı ya estimator yerine verilerek yapılıyor ya da estimator objesinin fit edildiği kısımda Pipeline fit edilerek gerçekleştiriliyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. GridSearch ve Pipeline ile Hiper Parametre Optimizasyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En iyi Hiper Parametreler:  {'estimator__max_depth': 10, 'estimator__n_estimators': 100}\n",
      "En iyi Skor:  0.962463402576237\n"
     ]
    }
   ],
   "source": [
    "# Pipeline'ımızı önceden oluşturduğumuz gibi oluşturuyoruz\n",
    "pipeline = Pipeline([\n",
    "        ('preprocesser', CustomPreprocesser()),\n",
    "        ('estimator', RandomForestClassifier(**{\"random_state\":42}))\n",
    "        ])\n",
    "\n",
    "# Parametre uzayımızı oluşturuyoruz, Hızlı bir örnek olması için az sayıda parametre ve arama uzayı yerleştirdim\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [50, 100],\n",
    "    'estimator__max_depth': [10, 20],\n",
    "}\n",
    "\n",
    "# GridSearchCV'nin estimator objesine pipeline objemizi verip kalan parametreleri veriyoruz\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, scoring='roc_auc')\n",
    "\n",
    "# Pipeline'ımız ihtiyaç duyduğu parametreleri Pipeline ile oluşturduğumuz grid_search objesinin .fit() fonksiyony içerisinde veriyoruz\n",
    "grid_search.fit(X, y, \n",
    "                preprocesser__scaler_standard=StandardScaler(), \n",
    "                preprocesser__scaler_min_max=MinMaxScaler())\n",
    "\n",
    "# Hiper parametre optimizasyon sonuçları\n",
    "print(\"En iyi Hiper Parametreler: \", grid_search.best_params_)\n",
    "print(\"En iyi Skor: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Optuna ve Pipeline ile Hiper Parametre Optimizasyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-08-12 08:43:22,594]\u001b[0m Trial 0 finished with value: 0.9610669317694911 and parameters: {'n_estimators': 81, 'max_depth': 17}. Best is trial 0 with value: 0.9610669317694911.\u001b[0m\n",
      "\u001b[32m[I 2024-08-12 08:43:37,440]\u001b[0m Trial 1 finished with value: 0.9625086335799332 and parameters: {'n_estimators': 69, 'max_depth': 13}. Best is trial 1 with value: 0.9625086335799332.\u001b[0m\n",
      "\u001b[32m[I 2024-08-12 08:43:48,236]\u001b[0m Trial 2 finished with value: 0.9621204407633593 and parameters: {'n_estimators': 62, 'max_depth': 10}. Best is trial 1 with value: 0.9625086335799332.\u001b[0m\n",
      "\u001b[32m[I 2024-08-12 08:43:57,749]\u001b[0m Trial 3 finished with value: 0.9619898665739092 and parameters: {'n_estimators': 54, 'max_depth': 10}. Best is trial 1 with value: 0.9625086335799332.\u001b[0m\n",
      "\u001b[32m[I 2024-08-12 08:44:09,344]\u001b[0m Trial 4 finished with value: 0.9615378590201438 and parameters: {'n_estimators': 50, 'max_depth': 15}. Best is trial 1 with value: 0.9625086335799332.\u001b[0m\n",
      "\u001b[32m[I 2024-08-12 08:44:21,910]\u001b[0m Trial 5 finished with value: 0.9613019306454103 and parameters: {'n_estimators': 51, 'max_depth': 16}. Best is trial 1 with value: 0.9625086335799332.\u001b[0m\n",
      "\u001b[32m[I 2024-08-12 08:44:44,183]\u001b[0m Trial 6 finished with value: 0.9600751132275999 and parameters: {'n_estimators': 84, 'max_depth': 20}. Best is trial 1 with value: 0.9625086335799332.\u001b[0m\n",
      "\u001b[32m[I 2024-08-12 08:44:59,106]\u001b[0m Trial 7 finished with value: 0.9596627371373333 and parameters: {'n_estimators': 57, 'max_depth': 20}. Best is trial 1 with value: 0.9625086335799332.\u001b[0m\n",
      "\u001b[32m[I 2024-08-12 08:45:09,901]\u001b[0m Trial 8 finished with value: 0.9626487592632397 and parameters: {'n_estimators': 51, 'max_depth': 13}. Best is trial 8 with value: 0.9626487592632397.\u001b[0m\n",
      "\u001b[32m[I 2024-08-12 08:45:26,739]\u001b[0m Trial 9 finished with value: 0.9626234756348516 and parameters: {'n_estimators': 90, 'max_depth': 11}. Best is trial 8 with value: 0.9626487592632397.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'n_estimators': 51, 'max_depth': 13}\n",
      "Best score:  0.9626487592632397\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    \n",
    "    # Pipeline objemizi oluşturuyoruz\n",
    "    ## Pipeline içerisinde estimator adımında bulunan modelimiz içerisine Optuna'nın trial.suggest formatlarıyla patametre uzayımızı ve ilgili parametreleri veriyoruz\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocesser', CustomPreprocesser()),\n",
    "        ('estimator', RandomForestClassifier(\n",
    "            n_estimators=trial.suggest_int('n_estimators', 50, 100),\n",
    "            max_depth=trial.suggest_int('max_depth', 10, 20),\n",
    "            **{\"random_state\":42}))\n",
    "        ])\n",
    "    \n",
    "    # Pipeline.fit() içerisine verdiğimiz parametreleri bir dictionary içerisine kaydedip daha sonra başka bir fonksiyon yardımı ile kullanacağız\n",
    "    fit_params = {\"preprocesser__scaler_standard\":StandardScaler(),\n",
    "                  \"preprocesser__scaler_min_max\":MinMaxScaler()}\n",
    "    \n",
    "    # Optuna'nın kendi içerisinde cross validation olmadığı için bu adımda daha önce örneğini yaptığımız cross_val_score kullanıyoruz\n",
    "    ## Pipeline.fit() fonksiyonuna verdiğimiz parametreleri fit_params içerisinde iletiyoruz\n",
    "    score = cross_val_score(pipeline, X, y, cv=3, scoring='roc_auc', fit_params=fit_params).mean()\n",
    "    return score\n",
    "\n",
    "# Optuna study'si oluşturup objective fonksiyonunda tanımladığımız Pipeline'ımızı optimize ediyoruz\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Hiper parametre optimizasyon sonuçları\n",
    "print(\"Best parameters found: \", study.best_params)\n",
    "print(\"Best score: \", study.best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paranormal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
